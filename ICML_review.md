# (IF I AM) ICML Reviewer Analysis: Agent-as-a-Judge
## Critical Evaluation for Accept/Reject Decision

---

## üéØ THE CRITICAL INSIGHT (Accept or Reject Hinges on This)

### **The Key Innovation: Tool-Augmented Gray-Box Evaluation**

The **single most important contribution** is demonstrating that **agentic systems with tools can evaluate other agentic systems better than pure LLM-as-a-Judge** by leveraging:

1. **Intermediate trajectory data** (gray-box) - not just final outputs
2. **Tool-based evidence gathering** (Graph, Locate, Read, Retrieve)
3. **Modular component design** with measurable ablations

### Why This Matters (Accept Arguments):

**üî• Novel Problem Formulation:**
- First to systematically evaluate *agentic systems* (multi-step reasoning + tools) rather than just single-shot LLM outputs
- Recognizes that agents should be judged on their *process* (trajectory), not just *outcomes*

**üìä Strong Empirical Validation:**
- 90% alignment with human consensus (vs 60-84% for LLM-as-a-Judge)
- Component ablation shows each piece contributes (Graph: +11%, Locate: +8%, Read: +6%)
- Works across multiple domains (CV, NLP, RL, Time Series)

**üí∞ Practical Impact:**
- 97.6% cost reduction ($25 ‚Üí $0.03 per task)
- 97.7% time reduction (2 hours ‚Üí 2 seconds)
- Enables rapid iteration for agent development

---

## ‚úÖ STRONGEST ARGUMENTS FOR ACCEPTANCE

### 1. **Paradigm Shift in Evaluation** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**The paper recognizes a fundamental shift:**
- **Old paradigm:** Evaluate LLM outputs (text generation)
- **New paradigm:** Evaluate agentic systems (reasoning + actions + tools)

Traditional metrics (BLEU, ROUGE, even LLM-as-a-Judge) fail for agents because:
- They only see final outputs
- They miss execution failures
- They can't assess multi-step reasoning
- They ignore tool use quality

**Agent-as-a-Judge solves this by evaluating the full trajectory.**

### 2. **Methodological Rigor** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**DevAI Dataset:**
- 55 realistic tasks (not toy problems)
- 365 hierarchical requirements with DAG structure
- Multiple annotators with consensus measurement
- Covers 5+ AI domains

**Proper Baselines:**
- Human consensus (gold standard)
- Individual human judges
- LLM-as-a-Judge variants
- Component ablations

**Reproducibility:**
- Clear methodology
- Dataset will be released
- Implementation details provided

### 3. **Component Ablation Study** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**This is CRUCIAL for acceptance:**

| Configuration | Alignment |
|--------------|-----------|
| Ask only (baseline) | 65% |
| + Graph | 76% (+11%) |
| + Read | 82% (+6%) |
| + Locate | 90% (+8%) |
| + Retrieve | 90% (varies) |

**Why this matters:**
- Proves each component is necessary
- Shows the system is not "just prompt engineering"
- Demonstrates modular design principles
- Validates the tool-use approach

### 4. **Gray-Box vs Black-Box** ‚≠ê‚≠ê‚≠ê‚≠ê
**Key insight:** Using trajectory data (gray-box) catches errors that black-box evaluation misses.

Example from paper:
- Agent generates perfect-looking code
- But trajectory shows "WARNING: normalization skipped"
- Black-box: ‚úì SATISFIED (file exists)
- Gray-box: ‚úó UNSATISFIED (implementation incomplete)

**This is the difference between:**
- "Did you produce output?" (black-box)
- "Did you do it correctly?" (gray-box)

### 5. **Hierarchical Requirements with Dependencies** ‚≠ê‚≠ê‚≠ê‚≠ê
**DAG structure is elegant:**
```
R0 (Load Data)
  ‚îú‚îÄ R1 (Preprocess) 
  ‚îÇ   ‚îî‚îÄ R3 (Train)
  ‚îÇ       ‚îî‚îÄ R5 (Evaluate)
  ‚îî‚îÄ R2 (Model)
      ‚îî‚îÄ R3 (Train)
```

If R0 fails, don't waste time evaluating R1, R3, R5.
**This is how humans think** - the paper captures this.

---

## ‚ùå STRONGEST ARGUMENTS FOR REJECTION

### 1. **Limited Scope - Only Code Generation** ‚≠ê‚≠ê‚≠ê
**Critical weakness:**
- Entire paper focuses on code-generating agents
- DevAI dataset is 100% development tasks
- No evaluation of dialogue agents, reasoning agents, robot agents

**Reviewer concern:**
"Does this generalize beyond code generation? The title says 'Evaluate Agents with Agents' but you only show code evaluation."

**Rebuttal:**
- Code is a good testbed (verifiable, structured)
- Framework is domain-agnostic
- But paper should be clearer about scope

### 2. **Can Be Fooled by Sophisticated Agents** ‚≠ê‚≠ê‚≠ê‚≠ê
**From paper's own limitations:**
- Synthetic data detection: Agent could create fake dataset that "looks right"
- Semantic nuances: Difference between "setting" vs "tuning" parameters
- Can't verify algorithm correctness without execution

**Example attack:**
```python
# Agent creates fake data to fool judge
def load_cifar10():
    # Actually generates random noise
    return create_fake_dataset(name="CIFAR-10")
```

**Reviewer concern:**
"This is not robust to adversarial agents. It's pattern matching, not understanding."

### 3. **High Variance in Retrieve Component** ‚≠ê‚≠ê‚≠ê
**From ablation study:**
- Retrieve (trajectory) has inconsistent gains
- Sometimes helps, sometimes doesn't
- Depends on agent's logging quality

**Reviewer concern:**
"If the core innovation is gray-box evaluation, why doesn't it consistently improve performance?"

**Counter:** The paper acknowledges this - it depends on trajectory informativeness.

### 4. **Cost Comparison May Be Unfair** ‚≠ê‚≠ê
**Human evaluation cost:**
- Paper claims $25 per task for humans
- But humans provide richer feedback
- Humans can fix obvious typos and re-evaluate
- Humans understand context better

**Agent-as-a-Judge cost:**
- $0.03 per task claimed
- But doesn't count:
    - Infrastructure setup
    - Prompt engineering time
    - Debugging false negatives/positives
    - Human oversight required

**Reviewer:** "Apples to oranges comparison."

### 5. **Reproducibility Concerns** ‚≠ê‚≠ê
**Issues:**
- Dataset "will be released" - not released yet
- No code repository URL in paper
- LLM versions matter (GPT-4 varies over time)
- Prompt templates not fully shown

**ICML standard:** Reproducibility is required for acceptance.

---

## ü§î THE DECIDING FACTORS

### Factor 1: **Is the core contribution novel enough?**

**YES ‚úì**
- First systematic study of agent-to-agent evaluation
- Tool-augmented evaluation is new
- Gray-box approach is under-explored
- DAG-structured requirements are innovative

### Factor 2: **Are the experiments convincing?**

**MOSTLY ‚úì**
- 55 tasks is reasonable (not huge, but adequate)
- Multiple domains shown
- Ablations prove necessity of components
- Human consensus baseline is appropriate

**BUT:**
- Would like to see more tasks (100+)
- Inter-annotator agreement details needed
- More statistical significance tests

### Factor 3: **Will this impact the field?**

**YES ‚úì‚úì‚úì**
- Agent development is exploding
- Evaluation is the bottleneck
- This provides a practical solution
- Will enable faster research iteration

### Factor 4: **Is it rigorous enough for ICML?**

**YES ‚úì**
- Proper experimental design
- Clear methodology
- Statistical analysis
- Ablation studies
- Baseline comparisons

---

## üìä MY RECOMMENDATION (As ICML Reviewer)

### **ACCEPT (Weak Accept / 6 out of 10)**

### Justification:

**Strengths:**
1. ‚úÖ Novel problem formulation (agent evaluation is timely)
2. ‚úÖ Strong empirical results (90% alignment)
3. ‚úÖ Rigorous ablations (each component justified)
4. ‚úÖ Practical impact (97% cost/time savings)
5. ‚úÖ Generalizable framework (works across domains)

**Weaknesses:**
1. ‚ö†Ô∏è Limited to code generation (narrow scope)
2. ‚ö†Ô∏è Can be fooled (not adversarially robust)
3. ‚ö†Ô∏è Trajectory component has high variance
4. ‚ö†Ô∏è Reproducibility concerns (dataset not released)
5. ‚ö†Ô∏è Some claims overclaimed (cost comparison)

### Why Not Strong Accept?
- Scope is narrower than title suggests
- Adversarial robustness not addressed
- Would like to see evaluation beyond code
- Some experimental details missing

### Why Not Reject?
- Core contribution is solid
- Results are convincing
- Ablations prove value
- Addresses real need
- Will impact field

---

## üéØ THE ONE CRITICAL POINT

If I had to pick **ONE POINT** that determines acceptance:

## **"Does tool-augmented evaluation (Agent-as-a-Judge) provide sufficient gains over prompt-engineering alone (LLM-as-a-Judge) to justify the added complexity?"**

### My Answer: **YES**

**Evidence:**
- 90% alignment vs 60-84% for LLM-as-a-Judge (20-30% improvement)
- Component ablations show it's not just one big prompt
- Gray-box evaluation catches errors black-box misses
- Works across multiple domains consistently

**The 20-30% alignment improvement** is the key metric.
- This is substantial in ML terms
- It's not marginal (1-2% would be reject)
- It's consistent across domains
- It's achieved through systematic design, not cherry-picking

---

## üí≠ CONDITIONAL ACCEPTANCE

### I would **ACCEPT** with these requirements:

**Required Changes:**
1. ‚úÖ Add explicit scope statement: "This paper focuses on code-generating agents"
2. ‚úÖ Discuss adversarial robustness limitations more prominently
3. ‚úÖ Provide dataset + code before camera-ready
4. ‚úÖ Add more inter-annotator agreement statistics
5. ‚úÖ Tone down cost comparison claims (add nuance)

**Optional Improvements:**
1. ‚≠ê Show evaluation on 1-2 non-code tasks (dialogue, reasoning)
2. ‚≠ê Analyze failure modes more systematically
3. ‚≠ê Compare to other tool-augmented approaches
4. ‚≠ê Provide confidence intervals on alignment metrics

---

## üìù REVIEW SUMMARY

### **Paper Decision: ACCEPT**

**Confidence: 4/5** (Quite confident)

**Summary for Meta-Review:**
"This paper introduces Agent-as-a-Judge, a tool-augmented framework for evaluating code-generating agentic systems. The key innovation is using component-based evidence gathering (Graph, Locate, Read, Retrieve, Ask) combined with gray-box trajectory analysis to achieve 90% alignment with human consensus - a 20-30% improvement over LLM-as-a-Judge baselines.

The strength lies in rigorous ablations proving each component's necessity, practical impact (97% cost/time savings), and a well-designed hierarchical requirement structure (DAG).

Weaknesses include limited scope (only code generation), potential vulnerability to adversarial agents, and incomplete reproducibility materials.

**Despite weaknesses, the core contribution is novel, timely, and experimentally validated. The paper addresses a critical bottleneck in agent development and will likely impact the field. I recommend acceptance with minor revisions.**"

---

## üî• IF I HAD TO GIVE ONE SENTENCE

**"The paper convincingly demonstrates that tool-augmented evaluation with access to execution trajectories (gray-box) significantly outperforms pure LLM-based evaluation, and this 20-30% improvement is substantial enough to justify acceptance despite scope limitations."**

---

## üéì FINAL VERDICT

### **ACCEPT**

**Reason:** The combination of:
1. Novel problem formulation
2. Strong empirical validation (90% alignment)
3. Rigorous ablations proving value
4. Practical impact
5. Timely contribution to agent evaluation

...outweighs the limitations of:
1. Narrow scope (code only)
2. Adversarial vulnerability
3. Reproducibility gaps

**This is good science addressing a real problem with a principled solution.**

The paper advances the field and deserves publication at ICML.

---

## ü§î META-QUESTION: What Would Make This a Strong Accept (8-9/10)?

1. **Evaluation beyond code** - Show 2-3 non-code tasks (dialogue, planning, reasoning)
2. **Adversarial analysis** - Test against intentionally deceptive agents
3. **Larger dataset** - 100+ tasks instead of 55
4. **User study** - Show developers prefer Agent-as-a-Judge feedback
5. **Deployment case study** - Real-world usage in agent development
6. **Theoretical analysis** - Why does this approach work? Sample complexity bounds?

**Without these: Accept (6/10)**
**With 2-3 of these: Strong Accept (8/10)**
**With all: Spotlight/Best Paper (9-10/10)**

---

*This analysis represents a balanced, critical evaluation considering both the paper's contributions and limitations from an ICML reviewer perspective.*